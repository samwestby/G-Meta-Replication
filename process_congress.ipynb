{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "87f2623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import dgl\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963efd1f",
   "metadata": {},
   "source": [
    "# Convert Congress dataset into DGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183be59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def convert_nx_to_dgl(graph):\n",
    "        K = dgl.DGLGraph()\n",
    "        digraph = nx.DiGraph(graph)\n",
    "        K.from_networkx(digraph)\n",
    "        return K\n",
    "\n",
    "dgl_list = []\n",
    "nx_list = []\n",
    "cc_indices = {}\n",
    "for i in range(93, 95):\n",
    "    print('Congress ', i)\n",
    "    path = '~/neu/gml/data/congress/edgelist_Congress'+str(i)+'.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(list(df['bill_id']), bipartite=0)\n",
    "    G.add_nodes_from(list(df['thomas_id']), bipartite=1)\n",
    "    edge_list = list(zip(df['bill_id'], df['thomas_id']))\n",
    "    G.add_edges_from(edge_list)\n",
    "    G = bipartite.weighted_projected_graph(G, list(df['bill_id']))\n",
    "    print(len(G), df['bill_id'].nunique())\n",
    "    nx_list.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e3c6a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_features = pd.read_csv('/home/sam/neu/gml/data/congress/original/bills_features_label.csv')\n",
    "congressman_features = pd.read_csv('/home/sam/neu/gml/data/congress/original/congressman_features_v2.csv')\n",
    "bills_passed = bill_features[bill_features['pass_law']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8cf30d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_majority_party(dataframe):\n",
    "    total = 0\n",
    "    majority_party_dict = {}\n",
    "    missing_data = set()\n",
    "    for bill in list(dataframe['bill_id'].unique()):\n",
    "        party = []\n",
    "        for t_id in dataframe[dataframe['bill_id']==bill]['thomas_id'].unique():\n",
    "            if t_id not in list(congressman_features['thomas_id']):\n",
    "                missing_data.add(t_id)\n",
    "                continue\n",
    "            party_options = congressman_features[congressman_features['thomas_id']==t_id]['party_code']\n",
    "\n",
    "            # If there are multiple parties, select a random one\n",
    "            party.append(np.random.choice(party_options))\n",
    "        # Let the majority party be the most common\n",
    "        if not len(party):\n",
    "            majority_party_dict[bill] = None\n",
    "            continue\n",
    "        majority_party_dict[bill] = Counter(party).most_common()[0][0]\n",
    "    print('    Missing data for: ',missing_data)\n",
    "    return majority_party_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "3c20682d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congress  93\n",
      "    131  nodes, 1017 edges\n",
      "\n",
      "Congress  94\n",
      "    98  nodes, 289 edges\n",
      "\n",
      "Congress  95\n",
      "    156  nodes, 412 edges\n",
      "\n",
      "Congress  96\n",
      "    197  nodes, 3120 edges\n",
      "\n",
      "Congress  97\n",
      "    94  nodes, 1722 edges\n",
      "\n",
      "Congress  98\n",
      "    113  nodes, 4722 edges\n",
      "\n",
      "Congress  99\n",
      "    370  nodes, 24718 edges\n",
      "\n",
      "Congress  100\n",
      "    403  nodes, 25702 edges\n",
      "\n",
      "Congress  101\n",
      "    359  nodes, 22873 edges\n",
      "\n",
      "Congress  102\n",
      "    301  nodes, 16459 edges\n",
      "\n",
      "Congress  103\n",
      "    183  nodes, 4831 edges\n",
      "\n",
      "Congress  104\n",
      "    109  nodes, 1123 edges\n",
      "\n",
      "Congress  105\n",
      "    101  nodes, 1303 edges\n",
      "\n",
      "Congress  106\n",
      "    249  nodes, 4525 edges\n",
      "\n",
      "Congress  107\n",
      "    166  nodes, 1863 edges\n",
      "\n",
      "Congress  108\n",
      "    225  nodes, 3501 edges\n",
      "\n",
      "Congress  109\n",
      "    210  nodes, 3190 edges\n",
      "\n",
      "Congress  110\n",
      "    252  nodes, 5190 edges\n",
      "\n",
      "Congress  111\n",
      "    192  nodes, 2615 edges\n",
      "\n",
      "Congress  112\n",
      "    116  nodes, 1363 edges\n",
      "\n",
      "Congress  113\n",
      "    145  nodes, 2073 edges\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nx_passed_list = []\n",
    "# label_dict_master = {}\n",
    "for i in range(93, 114):\n",
    "    print('Congress ', i)\n",
    "    path = '/home/sam/neu/gml/data/congress/original/edgelist_Congress'+str(i)+'.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df_passed = df[df['bill_id'].isin(bills_passed['bill_id'])]\n",
    "#     print('    creating labels')\n",
    "#     labels = add_majority_party(df_passed)\n",
    "#     label_dict_master[i] = labels\n",
    "#     path = '/home/sam/neu/gml/data/congress/pruned/labels/'+str(i)+'.json'\n",
    "#     with open(path, 'wb') as f:\n",
    "#         pickle.dump(labels, f)\n",
    "#     print('    labels written')\n",
    "#     print('    creating graph')\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(list(df_passed['bill_id'].unique()), bipartite=0)\n",
    "    G.add_nodes_from(list(df_passed['thomas_id'].unique()), bipartite=1)\n",
    "    edge_list = list(zip(df_passed['bill_id'], df_passed['thomas_id']))\n",
    "    G.add_edges_from(edge_list)\n",
    "    G = bipartite.weighted_projected_graph(G, list(df_passed['bill_id']))\n",
    "    # Remove edges with weight < 6\n",
    "    edge_list = list(G.edges(data=True))\n",
    "    for edge in edge_list:\n",
    "        if edge[2]['weight'] < 6:\n",
    "            G.remove_edge(edge[0], edge[1])\n",
    "    remove = [node for node,degree in dict(G.degree()).items() if degree < 1]\n",
    "    G.remove_nodes_from(remove)\n",
    "    print('   ',len(G), ' nodes,', len(G.edges), 'edges')\n",
    "    nx_passed_list.append(G)\n",
    "    path = '/home/sam/neu/gml/data/congress/pruned/graphs'+str(i)+'_graph.edgelist.gz'\n",
    "    nx.write_edgelist(G, path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3209c4a2",
   "metadata": {},
   "source": [
    "# Relabel nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "c1ea2d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:00, 127.04it/s]\n"
     ]
    }
   ],
   "source": [
    "def interpret_feature_string(fs):\n",
    "    return np.array([float(x) for x in fs[1:-1].split(', ')])\n",
    "\n",
    "path = '/home/sam/neu/gml/data/congress'\n",
    "dgl_gs = [None]*len(nx_passed_list)\n",
    "node_to_int = {}\n",
    "int_to_node = {}\n",
    "df_features = pd.read_csv(os.path.join(path, 'features_df.csv'))\n",
    "df_features = df_features.set_index('bill_id')\n",
    "feature_map = [None]*len(nx_passed_list)\n",
    "info = {} # labels dictionary\n",
    "for i, g in tqdm(enumerate(nx_passed_list)):\n",
    "#     Gcc = sorted(nx.connected_components(G0), key=len, reverse=True)\n",
    "#     g = G0.subgraph(Gcc[0])\n",
    "    congress_number = i+93\n",
    "    features = np.array([interpret_feature_string(\n",
    "                    df_features['vector'].loc[node]) for node in list(g.nodes)])\n",
    "    feature_map[i] = features\n",
    "    node_to_int[congress_number] = dict(zip(list(g.nodes), range(len(g.nodes))))\n",
    "    int_to_node[congress_number] = dict(zip(range(len(g.nodes)), list(g.nodes)))\n",
    "    for node in list(g.nodes):\n",
    "        party = label_dict_master[congress_number][node]\n",
    "        if party not in [100, 200]:\n",
    "            party = np.random.choice([100,200])\n",
    "        info[str(i)+'_'+str(node_to_int[congress_number][node])] = party\n",
    "    \n",
    "\n",
    "    g_relabeled = nx.relabel_nodes(g, node_to_int[congress_number])\n",
    "    digraph = nx.DiGraph(g_relabeled)\n",
    "    K = dgl.DGLGraph()\n",
    "    K.from_networkx(digraph)\n",
    "    dgl_gs[i] = K\n",
    "\n",
    "with open(path + '/pruned/graph_dgl.pkl', 'wb') as f:\n",
    "    pickle.dump(dgl_gs, f)\n",
    "\n",
    "with open(path + '/pruned/label.pkl', 'wb') as f:\n",
    "    pickle.dump(info, f)\n",
    "    \n",
    "with open(path + '/pruned/node_to_int.pkl', 'wb') as f:\n",
    "    pickle.dump(int_to_node, f)\n",
    "\n",
    "with open(path + '/pruned/int_to_node.pkl', 'wb') as f:\n",
    "    pickle.dump(int_to_node, f)\n",
    "    \n",
    "    \n",
    "np.save(path + '/pruned/features.npy', np.array(feature_map, dtype='object'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fb6c4c",
   "metadata": {},
   "source": [
    "# Create train, val, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "60382c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_labels = 2\n",
    "df = pd.DataFrame.from_dict(info, orient='index').reset_index().rename(columns={\"index\": \"name\", 0: \"label\"})\n",
    "val_graph = 2\n",
    "test_graph = 16\n",
    "train_graphs = list(range(len(dgl_gs)))\n",
    "train_graphs.remove(val_graph)\n",
    "train_graphs.remove(test_graph)\n",
    "\n",
    "\n",
    "val_df = df[df.name.str.contains(str(val_graph)+'_')]\n",
    "test_df = df[df.name.str.contains(str(test_graph)+'_')]\n",
    "\n",
    "train_df = df[~df.index.isin(val_df.index)]\n",
    "train_df = train_df[~train_df.index.isin(test_df.index)]\n",
    "train_df.reset_index(drop = True).to_csv(path + '/pruned/train.csv')\n",
    "val_df.reset_index(drop = True).to_csv(path + '/pruned/val.csv')\n",
    "test_df.reset_index(drop = True).to_csv(path + '/pruned/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c0daa",
   "metadata": {},
   "source": [
    "# Create feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download() # follow the prompts to download \"stopwords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag o words feature vector\n",
    "stops = stopwords.words('english')\n",
    "def text_preprocess(text:str):\n",
    "    # Ignoring case\n",
    "    text = text.lower()\n",
    "    # Ignoring punctuation\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Ignoring frequent words that don’t contain much information, called stop words, like “a,” “of,” etc.\n",
    "    text = text.split(' ')\n",
    "    text = [word for word in text if not word in stops]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def aggregate_features(x):\n",
    "    return [*x['word_bag'], *x['bill_type_vec'], *x['control_vec'], *x['pass_law_vec']]\n",
    "\n",
    "\n",
    "def one_hot_encoder(data:list):\n",
    "    n_types = len(set(data))\n",
    "    type_key = list(set(data))\n",
    "    encoded = [None]*len(data)\n",
    "    for i, x in enumerate(data):\n",
    "        vec = [0]*n_types\n",
    "        index = type_key.index(x)\n",
    "        vec[index] = 1\n",
    "        encoded[i] = vec\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecf9eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_features = pd.read_csv('/home/sam/neu/gml/data/congress/original/bills_features_label.csv')\n",
    "bill_features = bill_features\n",
    "clean_title_text = bill_features['title_text'].apply(text_preprocess)\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                            tokenizer = None,\n",
    "                            preprocessor = None,\n",
    "                            stop_words = None,\n",
    "                            max_features = 118)\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocaulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_title_text)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an array\n",
    "train_data_features = train_data_features.toarray()\n",
    "train_data_features_normed = normalize(train_data_features)\n",
    "print('Bag of words completed')\n",
    "bill_features['word_bag'] = list(train_data_features_normed)\n",
    "#  add party_control to the feature vector\n",
    "bill_features['control_vec'] = bill_features['party_control_congress'].apply(\n",
    "                                                    lambda x: [x/100-1])\n",
    "bill_features['bill_type_vec'] = one_hot_encoder(list(bill_features['bill_type']))\n",
    "bill_features['pass_law_vec'] = bill_features['pass_law'].apply(lambda x: [x])\n",
    "bill_features['vector'] = bill_features.apply(aggregate_features, axis=1)\n",
    "\n",
    "df_features = bill_features[['bill_id', 'vector']]\n",
    "with open('/home/sam/neu/gml/data/congress/original/features_df.csv', 'wb') as f:\n",
    "    df_features.to_csv(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
