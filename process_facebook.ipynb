{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import dgl\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "# Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/19201290/how-to-save-a-dictionary-to-a-file/32216025\n",
    "\n",
    "def save_list_or_dict_to_pkl(obj, obj_name):\n",
    "    pkl_output_file_name = obj_name + \".pkl\"\n",
    "    with open('obj/'+ pkl_output_file_name, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "    return \"saved \" + pkl_output_file_name\n",
    "\n",
    "def load_pkl_list_or_dict(obj_name):\n",
    "    pkl_file_name = obj_name + \".pkl\"\n",
    "    with open('obj/' + pkl_file_name, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "source": [
    "# Network to DGLGraph"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/sam/neu/gml/gmeta/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "edges_df = pd.read_csv(os.path.join(path, 'facebook_large/musae_facebook_edges.csv'))\n",
    "G_facebook = nx.Graph()\n",
    "G_facebook = nx.from_pandas_edgelist(edges_df, 'id_1', 'id_2')\n",
    "digraph = nx.DiGraph(G_facebook)\n",
    "K = dgl.DGLGraph()\n",
    "K.from_networkx(digraph)\n",
    "dgl_g = [K]\n",
    "with open(path + '/graph_dgl.pkl', 'wb') as f:\n",
    "    pickle.dump(dgl_g, f)"
   ]
  },
  {
   "source": [
    "# Rename labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id       facebook_id                                     page_name  \\\n",
       "0   0   145647315578475                      The Voice of China 中国好声音   \n",
       "1   1      191483281412                 U.S. Consulate General Mumbai   \n",
       "2   2   144761358898518                                          ESET   \n",
       "3   3   568700043198473  Consulate General of Switzerland in Montreal   \n",
       "4   4  1408935539376139             Mark Bailey MP - Labor for Miller   \n",
       "\n",
       "    page_type  page_type_int  \n",
       "0      tvshow              0  \n",
       "1  government              1  \n",
       "2     company              2  \n",
       "3  government              1  \n",
       "4  politician              3  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>facebook_id</th>\n      <th>page_name</th>\n      <th>page_type</th>\n      <th>page_type_int</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>145647315578475</td>\n      <td>The Voice of China 中国好声音</td>\n      <td>tvshow</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>191483281412</td>\n      <td>U.S. Consulate General Mumbai</td>\n      <td>government</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>144761358898518</td>\n      <td>ESET</td>\n      <td>company</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>568700043198473</td>\n      <td>Consulate General of Switzerland in Montreal</td>\n      <td>government</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1408935539376139</td>\n      <td>Mark Bailey MP - Labor for Miller</td>\n      <td>politician</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "path = '/home/sam/neu/gml/data/facebook'\n",
    "labels_df = pd.read_csv(os.path.join(path, 'facebook_large/musae_facebook_target.csv'))\n",
    "labels_key = ['tvshow', 'government', 'company', 'politician']\n",
    "labels_df['page_type_int'] = labels_df['page_type'].apply(lambda x: labels_key.index(x))\n",
    "labels_df['subgraph_node'] = labels_df['id'].apply(lambda x: '0_'+str(x))\n",
    "labels_dict = dict(zip(labels_df['subgraph_node'], labels_df['page_type_int']))\n",
    "with open(path + '/label.pkl', 'wb') as f:\n",
    "    pickle.dump(labels_dict, f)"
   ]
  },
  {
   "source": [
    "# Create network features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag o words feature vector\n",
    "stops = stopwords.words('english')\n",
    "def text_preprocess(text:str):\n",
    "    # Ignoring case\n",
    "    text = text.lower()\n",
    "    # Ignoring punctuation\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Ignoring frequent words that don’t contain much information, called stop words, like “a,” “of,” etc.\n",
    "    text = text.split(' ')\n",
    "    text = [word for word in text if not word in stops]\n",
    "    text = ' '.join(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bag of words completed\n"
     ]
    }
   ],
   "source": [
    "clean_title_text = labels_df['page_name'].apply(text_preprocess)\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                            tokenizer = None,\n",
    "                            preprocessor = None,\n",
    "                            stop_words = None,\n",
    "                            max_features = 256)\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocaulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_title_text)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an array\n",
    "train_data_features = train_data_features.toarray()\n",
    "train_data_features_normed = normalize(train_data_features)\n",
    "print('Bag of words completed')\n",
    "labels_df['vector'] = list(train_data_features_normed)\n",
    "\n",
    "\n",
    "features = np.array([np.array(x) for x in labels_df['vector']])\n",
    "np.save(path + '/features.npy', features)"
   ]
  },
  {
   "source": [
    "# Create train.csv, test.csv, and val.csv"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tvshow 3327\ngovernment 6880\ncompany 6495\npolitician 5768\n"
     ]
    }
   ],
   "source": [
    "for l in labels_df['page_type'].unique():\n",
    "    print(l, len(labels_df[labels_df['page_type'] == l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(labels_dict, orient='index').reset_index().rename(columns={\"index\": \"name\", 0: \"label\"})\n",
    "\n",
    "train_label = [2, 3]\n",
    "other_label = [0, 1]\n",
    "\n",
    "other_df = df[df['label'].isin(other_label)]\n",
    "# First half\n",
    "val_df = other_df.iloc[: int(len(other_df)/2)]\n",
    "# Second half\n",
    "test_df = other_df.iloc[int(len(other_df)/2) :]\n",
    "train_df = df[df['label'].isin(train_label)]\n",
    "\n",
    "train_df.reset_index(drop = True).to_csv(path + '/train.csv')\n",
    "val_df.reset_index(drop = True).to_csv(path + '/val.csv')\n",
    "test_df.reset_index(drop = True).to_csv(path + '/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          name  label\n",
       "0          0_0      0\n",
       "1          0_1      1\n",
       "3          0_3      1\n",
       "8          0_8      1\n",
       "9          0_9      1\n",
       "...        ...    ...\n",
       "22456  0_22456      1\n",
       "22460  0_22460      1\n",
       "22463  0_22463      1\n",
       "22467  0_22467      1\n",
       "22469  0_22469      0\n",
       "\n",
       "[10207 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0_0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0_1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0_3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0_8</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0_9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22456</th>\n      <td>0_22456</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>22460</th>\n      <td>0_22460</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>22463</th>\n      <td>0_22463</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>22467</th>\n      <td>0_22467</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>22469</th>\n      <td>0_22469</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10207 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "\n",
    "val_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}